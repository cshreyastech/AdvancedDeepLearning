{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requests.packages.urllib3.disable_warnings()\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn't support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html\n",
    "\"\"\"\n",
    "#Visualization of the filters of VGG16, via gradient ascent in input space.\n",
    "\n",
    "This script can run on CPU in a few minutes.\n",
    "\n",
    "Results example: ![Visualization](http://i.imgur.com/4nj4KjN.jpg)\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image as pil_image\n",
    "#from keras.preprocessing.image import save_img\n",
    "from keras import layers\n",
    "from keras.applications import vgg16\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1\n",
    "step = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"utility function to normalize a tensor.\n",
    "\n",
    "    # Arguments\n",
    "        x: An input tensor.\n",
    "\n",
    "    # Returns\n",
    "        The normalized input tensor.\n",
    "    \"\"\"\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    \"\"\"utility function to convert a float array into a valid uint8 image.\n",
    "\n",
    "    # Arguments\n",
    "        x: A numpy-array representing the generated image.\n",
    "\n",
    "    # Returns\n",
    "        A processed numpy-array, which could be used in e.g. imshow.\n",
    "    \"\"\"\n",
    "    # normalize tensor: center on 0., ensure std is 0.25\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.25\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(x, former):\n",
    "    \"\"\"utility function to convert a valid uint8 image back into a float array.\n",
    "       Reverses `deprocess_image`.\n",
    "\n",
    "    # Arguments\n",
    "        x: A numpy-array, which could be used in e.g. imshow.\n",
    "        former: The former numpy-array.\n",
    "                Need to determine the former mean and variance.\n",
    "\n",
    "    # Returns\n",
    "        A processed numpy-array representing the generated image.\n",
    "    \"\"\"\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((2, 0, 1))\n",
    "    return (x / 255 - 0.5) * 4 * former.std() + former.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer(model,\n",
    "                    layer_name,\n",
    "                    step=1.,\n",
    "                    epochs=15,\n",
    "                    upscaling_steps=9,\n",
    "                    upscaling_factor=1.2,\n",
    "                    output_dim=(412, 412),\n",
    "                    filter_range=(0, None)):\n",
    "    \"\"\"Visualizes the most relevant filters of one conv-layer in a certain model.\n",
    "\n",
    "    # Arguments\n",
    "        model: The model containing layer_name.\n",
    "        layer_name: The name of the layer to be visualized.\n",
    "                    Has to be a part of model.\n",
    "        step: step size for gradient ascent.\n",
    "        epochs: Number of iterations for gradient ascent.\n",
    "        upscaling_steps: Number of upscaling steps.\n",
    "                         Starting image is in this case (80, 80).\n",
    "        upscaling_factor: Factor to which to slowly upgrade\n",
    "                          the image towards output_dim.\n",
    "        output_dim: [img_width, img_height] The output image dimensions.\n",
    "        filter_range: Tupel[lower, upper]\n",
    "                      Determines the to be computed filter numbers.\n",
    "                      If the second value is `None`,\n",
    "                      the last filter will be inferred as the upper boundary.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_filter_image(input_img,\n",
    "                           layer_output,\n",
    "                           filter_index):\n",
    "    \"\"\"Generates image for one particular filter.\n",
    "\n",
    "    # Arguments\n",
    "        input_img: The input-image Tensor.\n",
    "        layer_output: The output-image Tensor.\n",
    "        filter_index: The to be processed filter number.\n",
    "                      Assumed to be valid.\n",
    "\n",
    "    #Returns\n",
    "        Either None if no image could be generated.\n",
    "        or a tuple of the image (array) itself and the last loss.\n",
    "    \"\"\"\n",
    "    s_time = time.time()\n",
    "\n",
    "    # we build a loss function that maximizes the activation\n",
    "    # of the nth filter of the layer considered\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "    else:\n",
    "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "    # we compute the gradient of the input picture wrt this loss\n",
    "    grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "    # normalization trick: we normalize the gradient\n",
    "    grads = normalize(grads)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "    # we start from a gray image with some random noise\n",
    "    intermediate_dim = tuple(\n",
    "        int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim)\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_img_data = np.random.random(\n",
    "            (1, 3, intermediate_dim[0], intermediate_dim[1]))\n",
    "    else:\n",
    "        input_img_data = np.random.random(\n",
    "            (1, intermediate_dim[0], intermediate_dim[1], 3))\n",
    "    input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "    # Slowly upscaling towards the original size prevents\n",
    "    # a dominating high-frequency of the to visualized structure\n",
    "    # as it would occur if we directly compute the 412d-image.\n",
    "    # Behaves as a better starting point for each following dimension\n",
    "    # and therefore avoids poor local minima\n",
    "    for up in reversed(range(upscaling_steps)):\n",
    "        # we run gradient ascent for e.g. 20 steps\n",
    "        for _ in range(epochs):\n",
    "            loss_value, grads_value = iterate([input_img_data])\n",
    "            input_img_data += grads_value * step\n",
    "\n",
    "            # some filters get stuck to 0, we can skip them\n",
    "            if loss_value <= K.epsilon():\n",
    "                return None\n",
    "\n",
    "        # Calculate upscaled dimension\n",
    "        intermediate_dim = tuple(\n",
    "            int(x / (upscaling_factor ** up)) for x in output_dim)\n",
    "        # Upscale\n",
    "        img = deprocess_image(input_img_data[0])\n",
    "        img = np.array(pil_image.fromarray(img).resize(intermediate_dim,\n",
    "                                                       pil_image.BICUBIC))\n",
    "        input_img_data = np.expand_dims(\n",
    "            process_image(img, input_img_data[0]), 0)\n",
    "\n",
    "    # decode the resulting input image\n",
    "    img = deprocess_image(input_img_data[0])\n",
    "    e_time = time.time()\n",
    "    print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index,\n",
    "                                                              loss_value,\n",
    "                                                              e_time - s_time))\n",
    "    return img, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img(img, fname):\n",
    "    pil_img = deprocess_image(np.copy(img))\n",
    "    scipy.misc.imsave(fname, pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute filters 0 to 512\n",
      "Costs of filter   4:     1 ( 4.86s )\n",
      "Costs of filter  36:     1 ( 4.97s )\n",
      "Costs of filter  42:     1 ( 4.94s )\n",
      "Costs of filter  46:     1 ( 4.84s )\n",
      "Costs of filter  55:     1 ( 4.86s )\n",
      "Costs of filter  56:     1 ( 4.95s )\n",
      "Costs of filter  76:     1 ( 5.15s )\n",
      "Costs of filter  82:     0 ( 5.44s )\n",
      "Costs of filter  99:     1 ( 5.22s )\n",
      "Costs of filter 108:     0 ( 5.45s )\n",
      "Costs of filter 118:     1 ( 5.27s )\n",
      "Costs of filter 119:     1 ( 5.42s )\n",
      "Costs of filter 123:     1 ( 5.50s )\n",
      "Costs of filter 133:     1 ( 5.45s )\n",
      "Costs of filter 143:     0 ( 5.53s )\n",
      "Costs of filter 155:    10 ( 5.54s )\n",
      "Costs of filter 157:     0 ( 5.70s )\n",
      "Costs of filter 172:     1 ( 5.54s )\n",
      "Costs of filter 179:     1 ( 6.11s )\n",
      "Costs of filter 185:     0 ( 5.88s )\n",
      "Costs of filter 186:     1 ( 6.04s )\n",
      "Costs of filter 199:     1 ( 5.86s )\n",
      "Costs of filter 229:     1 ( 5.93s )\n",
      "Costs of filter 232:     0 ( 6.24s )\n",
      "Costs of filter 237:     0 ( 6.01s )\n",
      "Costs of filter 248:     1 ( 6.17s )\n",
      "Costs of filter 250:     0 ( 6.33s )\n",
      "Costs of filter 254:     1 ( 6.37s )\n",
      "Costs of filter 259:     1 ( 6.26s )\n",
      "Costs of filter 276:     1 ( 6.36s )\n",
      "Costs of filter 315:     2 ( 6.62s )\n",
      "Costs of filter 317:     1 ( 6.73s )\n",
      "Costs of filter 319:     1 ( 6.83s )\n",
      "Costs of filter 321:     0 ( 6.68s )\n",
      "Costs of filter 325:     1 ( 7.06s )\n",
      "Costs of filter 326:     1 ( 6.72s )\n",
      "Costs of filter 328:     1 ( 6.72s )\n",
      "Costs of filter 335:     0 ( 6.72s )\n",
      "Costs of filter 343:     1 ( 6.93s )\n",
      "Costs of filter 352:     1 ( 6.82s )\n",
      "Costs of filter 356:     0 ( 6.85s )\n",
      "Costs of filter 369:     2 ( 6.87s )\n",
      "Costs of filter 381:     2 ( 7.17s )\n",
      "Costs of filter 382:     1 ( 7.40s )\n",
      "Costs of filter 416:     0 ( 7.36s )\n",
      "Costs of filter 417:     1 ( 8.11s )\n",
      "Costs of filter 432:     0 ( 8.20s )\n",
      "Costs of filter 438:     1 ( 7.72s )\n",
      "Costs of filter 440:     0 ( 7.90s )\n",
      "Costs of filter 441:     4 ( 10.32s )\n",
      "Costs of filter 444:     1 ( 7.60s )\n",
      "Costs of filter 450:     0 ( 7.62s )\n",
      "Costs of filter 464:     1 ( 7.81s )\n",
      "Costs of filter 483:     0 ( 7.93s )\n",
      "Costs of filter 486:     0 ( 7.99s )\n",
      "Costs of filter 491:     1 ( 7.82s )\n",
      "Costs of filter 498:     1 ( 8.28s )\n",
      "Costs of filter 499:     1 ( 8.17s )\n",
      "Costs of filter 500:     1 ( 8.36s )\n",
      "Costs of filter 502:     2 ( 8.06s )\n",
      "Costs of filter 504:     1 ( 8.07s )\n",
      "61 filter processed.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot perform reduce with flexible type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-b61d964fc840>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} filter processed.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_filters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# Finally draw and store the best filters to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0m_draw_filters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_filters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-b61d964fc840>\u001b[0m in \u001b[0;36m_draw_filters\u001b[0;34m(filters, n)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# save the result to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0msave_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vgg_{0:}_{1:}x{1:}.png'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstitched_filters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# this is the placeholder for the input images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-a6d2f4e11f68>\u001b[0m in \u001b[0;36msave_img\u001b[0;34m(img, fname)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpil_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-a417c70fb109>\u001b[0m in \u001b[0;36mdeprocess_image\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# normalize tensor: center on 0., ensure std is 0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kerasenv/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot perform reduce with flexible type"
     ]
    }
   ],
   "source": [
    "def _draw_filters(filters, n=None):\n",
    "    \"\"\"Draw the best filters in a nxn grid.\n",
    "\n",
    "    # Arguments\n",
    "        filters: A List of generated images and their corresponding losses\n",
    "                 for each processed filter.\n",
    "        n: dimension of the grid.\n",
    "           If none, the largest possible square will be used\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = int(np.floor(np.sqrt(len(filters))))\n",
    "\n",
    "    # the filters that have the highest loss are assumed to be better-looking.\n",
    "    # we will only keep the top n*n filters.\n",
    "    filters.sort(key=lambda x: x[1], reverse=True)\n",
    "    filters = filters[:n * n]\n",
    "\n",
    "    # build a black picture with enough space for\n",
    "    # e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between\n",
    "    MARGIN = 5\n",
    "    width = n * output_dim[0] + (n - 1) * MARGIN\n",
    "    height = n * output_dim[1] + (n - 1) * MARGIN\n",
    "    stitched_filters = np.zeros((width, height, 3), dtype='uint8')\n",
    "\n",
    "    # fill the picture with our saved filters\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            img, _ = filters[i * n + j]\n",
    "            width_margin = (output_dim[0] + MARGIN) * i\n",
    "            height_margin = (output_dim[1] + MARGIN) * j\n",
    "            stitched_filters[\n",
    "                width_margin: width_margin + output_dim[0],\n",
    "                height_margin: height_margin + output_dim[1], :] = img\n",
    "\n",
    "    # save the result to disk\n",
    "    save_img('vgg_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters)\n",
    "\n",
    "# this is the placeholder for the input images\n",
    "assert len(model.inputs) == 1\n",
    "input_img = model.inputs[0]\n",
    "\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "\n",
    "\n",
    "output_layer = layer_dict[layer_name]\n",
    "assert isinstance(output_layer, layers.Conv2D)\n",
    "\n",
    "filter_range=(0, None)\n",
    "output_dim=(412, 412)\n",
    "upscaling_factor=1.2\n",
    "upscaling_steps=9\n",
    "\n",
    "# Compute to be processed filter range\n",
    "filter_lower = filter_range[0]\n",
    "filter_upper = (filter_range[1]\n",
    "                if filter_range[1] is not None\n",
    "                else len(output_layer.get_weights()[1]))\n",
    "\n",
    "assert(filter_lower >= 0\n",
    "       and filter_upper <= len(output_layer.get_weights()[1])\n",
    "       and filter_upper > filter_lower)\n",
    "print('Compute filters {:} to {:}'.format(filter_lower, filter_upper))\n",
    "\n",
    "# iterate through each filter and generate its corresponding image\n",
    "processed_filters = []\n",
    "for f in range(filter_lower, filter_upper):\n",
    "    img_loss = _generate_filter_image(input_img, output_layer.output, f)\n",
    "\n",
    "    if img_loss is not None:\n",
    "        processed_filters.append(img_loss)\n",
    "\n",
    "print('{} filter processed.'.format(len(processed_filters)))\n",
    "# Finally draw and store the best filters to disk\n",
    "_draw_filters(processed_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # the name of the layer we want to visualize\n",
    "    # (see model definition at keras/applications/vgg16.py)\n",
    "    LAYER_NAME = 'block5_conv1'\n",
    "\n",
    "    # build the VGG16 network with ImageNet weights\n",
    "    vgg = vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "    print('Model loaded.')\n",
    "    vgg.summary()\n",
    "\n",
    "    # example function call\n",
    "    visualize_layer(vgg, LAYER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
